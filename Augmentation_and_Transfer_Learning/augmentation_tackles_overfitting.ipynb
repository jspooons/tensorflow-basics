{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-11 02:40:05--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
      "Resolving download.microsoft.com (download.microsoft.com)... 23.42.152.252\n",
      "Connecting to download.microsoft.com (download.microsoft.com)|23.42.152.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 824887076 (787M) [application/octet-stream]\n",
      "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
      "\n",
      "/tmp/cats-and-dogs. 100%[===================>] 786.67M  54.5MB/s    in 14s     \n",
      "\n",
      "2023-11-11 02:40:19 (56.2 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824887076/824887076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" \\\n",
    "    -O \"/tmp/cats-and-dogs.zip\"\n",
    "\n",
    "local_zip = '/tmp/cats-and-dogs.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./data')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Remove .db files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '/data/PetImages'\n",
    "\n",
    "source_path_dogs = os.path.join(source_path, 'Dog')\n",
    "source_path_cats = os.path.join(source_path, 'Cat')\n",
    "\n",
    "# Deletes all non-image files (there are two .db files bundled into the dataset)\n",
    "!find ./data/PetImages/ -type f ! -name \"*.jpg\" -exec rm {} +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Format Data into Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/cats-v-dogs'\n",
    "\n",
    "# Empty directory to prevent FileExistsError is the function is run several times\n",
    "if os.path.exists(root_dir):\n",
    "    shutil.rmtree(root_dir)\n",
    "\n",
    "# Creates directories for the train and test sets\n",
    "def create_train_val_dirs(root_path):\n",
    "    os.makedirs(root_path)\n",
    "\n",
    "    train_dir = os.path.join(root_path, 'training')\n",
    "    validation_dir = os.path.join(root_path, 'validation')\n",
    "\n",
    "    train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "    train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "    validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "    validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "    os.makedirs(train_cats_dir)\n",
    "    os.makedirs(train_dogs_dir)\n",
    "\n",
    "    os.makedirs(validation_cats_dir)\n",
    "    os.makedirs(validation_dogs_dir)\n",
    "\n",
    "create_train_val_dirs(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source_dir, training_dir, validation_dir, split_size):\n",
    "    files = os.listdir(source_dir)\n",
    "\n",
    "    non_zero_files = [file for file in files if os.path.getsize(os.path.join(source_dir, file)) > 0]\n",
    "    zero_files = [file for file in files if os.path.getsize(os.path.join(source_dir, file)) == 0]\n",
    "\n",
    "    for file in zero_files:\n",
    "        print(f'{file} is zero length, so ignoring')\n",
    "\n",
    "    num_files = len(non_zero_files)\n",
    "    shuffled = random.sample(non_zero_files, num_files)\n",
    "\n",
    "    num_training = int(num_files * split_size)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        path = os.path.join(source_dir, shuffled[i])\n",
    "\n",
    "        if (i < num_training):\n",
    "            new_path = os.path.join(training_dir, shuffled[i])\n",
    "            copyfile(path, new_path)\n",
    "        else:\n",
    "            new_path = os.path.join(validation_dir, shuffled[i])\n",
    "            copyfile(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666.jpg is zero length, so ignoring\n",
      "11702.jpg is zero length, so ignoring\n",
      "There are 11249 images of cats for training\n",
      "There are 11249 images of dogs for training\n",
      "There are 1250 images of cats for validation\n",
      "There are 1250 images of dogs for validation\n"
     ]
    }
   ],
   "source": [
    "split_data(\"./data/PetImages/Cat/\", \"./data/cats-v-dogs/training/cats/\", \"./data/cats-v-dogs/validation/cats/\", .9)\n",
    "split_data(\"./data/PetImages/Dog/\", \"./data/cats-v-dogs/training/dogs/\", \"./data/cats-v-dogs/validation/dogs/\", .9)\n",
    "\n",
    "print(f\"There are {len(os.listdir('./data/cats-v-dogs/training/cats/'))} images of cats for training\")\n",
    "print(f\"There are {len(os.listdir('./data/cats-v-dogs/training/dogs/'))} images of dogs for training\")\n",
    "print(f\"There are {len(os.listdir('./data/cats-v-dogs/validation/cats/'))} images of cats for validation\")\n",
    "print(f\"There are {len(os.listdir('./data/cats-v-dogs/validation/dogs/'))} images of dogs for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_generators(training_dir, validation_dir):\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       fill_mode='nearest')\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(directory=training_dir,\n",
    "                                                        batch_size=128,\n",
    "                                                        class_mode='binary',\n",
    "                                                        target_size=(150,150))\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(directory=validation_dir,\n",
    "                                                                  batch_size=32,\n",
    "                                                                  class_mode='binary',\n",
    "                                                                  target_size=(150,150))\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22498 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = train_val_generators('./data/cats-v-dogs/training/', './data/cats-v-dogs/validation/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "124/176 [====================>.........] - ETA: 29s - loss: 0.6888 - accuracy: 0.5528"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 105s 591ms/step - loss: 0.6832 - accuracy: 0.5693 - val_loss: 0.6346 - val_accuracy: 0.6616\n",
      "Epoch 2/15\n",
      "176/176 [==============================] - 107s 604ms/step - loss: 0.6442 - accuracy: 0.6297 - val_loss: 0.6916 - val_accuracy: 0.6080\n",
      "Epoch 3/15\n",
      "176/176 [==============================] - 102s 576ms/step - loss: 0.6148 - accuracy: 0.6621 - val_loss: 0.5674 - val_accuracy: 0.7252\n",
      "Epoch 4/15\n",
      "176/176 [==============================] - 99s 561ms/step - loss: 0.5872 - accuracy: 0.6901 - val_loss: 0.5943 - val_accuracy: 0.6588\n",
      "Epoch 5/15\n",
      "176/176 [==============================] - 101s 572ms/step - loss: 0.5630 - accuracy: 0.7076 - val_loss: 0.5278 - val_accuracy: 0.7292\n",
      "Epoch 6/15\n",
      "176/176 [==============================] - 102s 579ms/step - loss: 0.5489 - accuracy: 0.7219 - val_loss: 0.4543 - val_accuracy: 0.7844\n",
      "Epoch 7/15\n",
      "176/176 [==============================] - 103s 585ms/step - loss: 0.5250 - accuracy: 0.7394 - val_loss: 0.4546 - val_accuracy: 0.7812\n",
      "Epoch 8/15\n",
      "176/176 [==============================] - 104s 587ms/step - loss: 0.5065 - accuracy: 0.7558 - val_loss: 0.5317 - val_accuracy: 0.7180\n",
      "Epoch 9/15\n",
      "176/176 [==============================] - 105s 592ms/step - loss: 0.4914 - accuracy: 0.7606 - val_loss: 0.3978 - val_accuracy: 0.8176\n",
      "Epoch 10/15\n",
      "176/176 [==============================] - 105s 593ms/step - loss: 0.4683 - accuracy: 0.7766 - val_loss: 0.4032 - val_accuracy: 0.8136\n",
      "Epoch 11/15\n",
      "176/176 [==============================] - 106s 601ms/step - loss: 0.4563 - accuracy: 0.7846 - val_loss: 0.3752 - val_accuracy: 0.8368\n",
      "Epoch 12/15\n",
      "176/176 [==============================] - 106s 600ms/step - loss: 0.4426 - accuracy: 0.7906 - val_loss: 0.4725 - val_accuracy: 0.7736\n",
      "Epoch 13/15\n",
      "176/176 [==============================] - 105s 593ms/step - loss: 0.4253 - accuracy: 0.8033 - val_loss: 0.3172 - val_accuracy: 0.8584\n",
      "Epoch 14/15\n",
      "176/176 [==============================] - 103s 586ms/step - loss: 0.4152 - accuracy: 0.8086 - val_loss: 0.3255 - val_accuracy: 0.8576\n",
      "Epoch 15/15\n",
      "176/176 [==============================] - 109s 619ms/step - loss: 0.4053 - accuracy: 0.8124 - val_loss: 0.3450 - val_accuracy: 0.8568\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "history = model.fit(train_generator, epochs=15, verbose=1, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a list of list results on training and test data sets for each training epoch\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc))\n",
    "\n",
    "# plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
